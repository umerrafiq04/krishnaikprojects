{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba94673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\ml_krishp\\venv2\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\ml_krishp\\venv2\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 409.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.5/1.5 MB 409.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.5/1.5 MB 409.0 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.5 MB 414.1 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 414.1 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 414.1 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 423.0 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 423.0 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 423.0 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 409.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 409.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 412.2 kB/s eta 0:00:00\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bbd36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4fbcc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14897ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hek\n"
     ]
    }
   ],
   "source": [
    "print(\"hek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbeebabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08a24858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Umer',\n",
       " 'is',\n",
       " 'learning.learning',\n",
       " 'nlp',\n",
       " 'from',\n",
       " 'basics',\n",
       " '!',\n",
       " 'HE',\n",
       " 'is',\n",
       " 'genuius']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"Umer is learning.learning nlp from basics ! HE is genuius\" # He is doing great!\"\n",
    "\n",
    "word_tokenize(corpus)  # ['Umer', 'is', 'learning', 'NLP', '.', 'He', 'is', 'doing', 'great', '!']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1abf3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Umer is learning.learning nlp from basics !', 'HE is genuius']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sent_tokenize(corpus))  # ['Umer is learning NLP.', 'He is doing great!']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24bb2000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "documents=sent_tokenize(corpus)\n",
    "print(type(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umer is learning.learning nlp from basics !\n",
      "HE is genuius\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a536c1c",
   "metadata": {},
   "source": [
    "* tokenization\n",
    "* paragraph to words\n",
    "* sentence to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5dc098a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Umer', 'is', 'learning.learning', 'nlp', 'from', 'basics', '!', 'HE', 'is', 'genuius']\n",
      "['!' 'HE' 'Umer' 'basics' 'from' 'genuius' 'is' 'learning.learning' 'nlp']\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(corpus)\n",
    "import numpy as np\n",
    "print(words)\n",
    "print(np.unique(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f11e4357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Umer', 'is', 'learning.learning', 'nlp', 'from', 'basics', '!']\n",
      "['HE', 'is', 'genuius']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    # Tokenize the sentence\n",
    "    print(word_tokenize(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c13532",
   "metadata": {},
   "source": [
    "* another library\n",
    "* umer's\n",
    "* 's gets seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8cfbff66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey',\n",
       " 'this',\n",
       " 'is',\n",
       " 'umer',\n",
       " 'here',\n",
       " '!',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'using',\n",
       " 'sahils',\n",
       " \"'\",\n",
       " 's',\n",
       " 'laptop',\n",
       " 'roight',\n",
       " 'now']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=\"hey this is umer here! and i'm using sahils's laptop roight now\"\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d8ec2",
   "metadata": {},
   "source": [
    "fullstop will not be treated as seperate except last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "114cd9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fullstop.',\n",
       " 'will',\n",
       " 'not',\n",
       " 'be',\n",
       " 'treated',\n",
       " 'as',\n",
       " 'seperate',\n",
       " 'except',\n",
       " 'last',\n",
       " 'one',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=\"fullstop. will not be treated as seperate except last one.\"\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb0de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe55bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe365e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
